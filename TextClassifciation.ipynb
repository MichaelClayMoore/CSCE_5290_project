{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassifciation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNdAClNwEBEt",
        "outputId": "9e69016b-b273-4c02-d68f-f86383f425d0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNwJTg_gv3aa",
        "outputId": "19649a70-4a09-4cd6-8f0c-1dd5b8a83c6b"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aah5KWQv5u6",
        "outputId": "90352693-d8a7-4ca9-af0a-ced9b0cee899"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIKbVq2cJKV-",
        "outputId": "a1fbe492-de0b-4fae-c237-64ccc1f63122"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S3N6pY4ECM5"
      },
      "source": [
        "import pandas as pd\n",
        "# because the dataset is int tsv format we have to use delimeter.\n",
        "df = pd.read_json('/content/drive/MyDrive/UNT/5290 Reviews Project/Musical_Instruments.json', lines=True)\n",
        "df = df.head(800)\n",
        "test = df.tail(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7lCmF9Njc01"
      },
      "source": [
        "df2 = pd.read_csv('/content/drive/MyDrive/UNT/5290 Reviews Project/output.csv')\n",
        "df2 = df2.head(800)\n",
        "test2 = df2.tail(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "ypGS6-e0KSeD",
        "outputId": "69b01fc7-0723-497a-ed17-610097285450"
      },
      "source": [
        "# creating a copy so we don't messed up our original dataset.\n",
        "data=df.copy()\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>vote</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>False</td>\n",
              "      <td>08 9, 2004</td>\n",
              "      <td>AXHY24HWOF184</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Bendy</td>\n",
              "      <td>Crocheting for Dummies by Karen Manthey &amp; Susa...</td>\n",
              "      <td>Terrific Book for Learning the Art of Crochet</td>\n",
              "      <td>1092009600</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>04 6, 2017</td>\n",
              "      <td>A29OWR79AM796H</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>Amazon Customer</td>\n",
              "      <td>Very helpful...</td>\n",
              "      <td>Four Stars</td>\n",
              "      <td>1491436800</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>03 14, 2017</td>\n",
              "      <td>AUPWU27A7X5F6</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Amazon Customer</td>\n",
              "      <td>EASY TO UNDERSTAND AND A PROMPT SERVICE TOO</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1489449600</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>02 14, 2017</td>\n",
              "      <td>A1N69A47D4JO6K</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Christopher Burnett</td>\n",
              "      <td>My girlfriend use quite often</td>\n",
              "      <td>Four Stars</td>\n",
              "      <td>1487030400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>01 29, 2017</td>\n",
              "      <td>AHTIQUMVCGBFJ</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Amazon Customer</td>\n",
              "      <td>Arrived as described. Very happy.</td>\n",
              "      <td>Very happy.</td>\n",
              "      <td>1485648000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>01 4, 2017</td>\n",
              "      <td>A1J8LQ7HVLR9GU</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Kindle Edition'}</td>\n",
              "      <td>Iheartmanatees</td>\n",
              "      <td>Love the Dummies Series.  Never fails.</td>\n",
              "      <td>Love the Dummies Series</td>\n",
              "      <td>1483488000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>01 2, 2017</td>\n",
              "      <td>ABVTZ63S6GOWF</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>D. Eva</td>\n",
              "      <td>Good book.</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1483315200</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>12 21, 2016</td>\n",
              "      <td>A2HX9NFBXGSWRW</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Stoeffels</td>\n",
              "      <td>Just started reading it. Love the charts &amp; cau...</td>\n",
              "      <td>Clear. Good reminders.</td>\n",
              "      <td>1482278400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>12 20, 2016</td>\n",
              "      <td>AP1TQR64HQRCI</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>nan ekelund</td>\n",
              "      <td>GREAT  book</td>\n",
              "      <td>Four Stars</td>\n",
              "      <td>1482192000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>12 15, 2016</td>\n",
              "      <td>A37FC9MED20AO</td>\n",
              "      <td>0470536454</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Jacqueline Bryant</td>\n",
              "      <td>this is a very helpful book.</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1481760000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall vote  ...  unixReviewTime image\n",
              "0        5   90  ...      1092009600   NaN\n",
              "1        4    2  ...      1491436800   NaN\n",
              "2        5  NaN  ...      1489449600   NaN\n",
              "3        4  NaN  ...      1487030400   NaN\n",
              "4        5  NaN  ...      1485648000   NaN\n",
              "5        5  NaN  ...      1483488000   NaN\n",
              "6        5  NaN  ...      1483315200   NaN\n",
              "7        4  NaN  ...      1482278400   NaN\n",
              "8        4  NaN  ...      1482192000   NaN\n",
              "9        5  NaN  ...      1481760000   NaN\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "FqcakMEnkFDY",
        "outputId": "2241cec6-03da-4e9b-e243-1342c52d2899"
      },
      "source": [
        "data2=df2.copy()\n",
        "data2[\"label\"] = '1'\n",
        "data2.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This case is a great value. I bought this for...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The best!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I've put this windscreen for over 6 months an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Just what I needed.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nice for the price.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>This guitar is built like a tank and is very ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I bought this guitar because of the price of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Item as expected.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I bought this item and it works great. The on...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>This is a great product.  It has a very good ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          reviewText label\n",
              "0   This case is a great value. I bought this for...     1\n",
              "1                                          The best!     1\n",
              "2   I've put this windscreen for over 6 months an...     1\n",
              "3                                Just what I needed.     1\n",
              "4                                Nice for the price.     1\n",
              "5   This guitar is built like a tank and is very ...     1\n",
              "6   I bought this guitar because of the price of ...     1\n",
              "7                                  Item as expected.     1\n",
              "8   I bought this item and it works great. The on...     1\n",
              "9   This is a great product.  It has a very good ...     1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "on8ihc4BEKdK",
        "outputId": "355073cc-c451-4060-91e7-29d80f5bcfa4"
      },
      "source": [
        "data.drop(['overall',\t'vote',\t'verified',\t'reviewTime',\t'reviewerID',\t'asin',\t'style',\t'reviewerName',\t'summary'\t,'unixReviewTime' ,\t'image'],axis=1,inplace=True)\n",
        "data[\"label\"] = '0'\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Crocheting for Dummies by Karen Manthey &amp; Susa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very helpful...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EASY TO UNDERSTAND AND A PROMPT SERVICE TOO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My girlfriend use quite often</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Arrived as described. Very happy.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          reviewText label\n",
              "0  Crocheting for Dummies by Karen Manthey & Susa...     0\n",
              "1                                    Very helpful...     0\n",
              "2        EASY TO UNDERSTAND AND A PROMPT SERVICE TOO     0\n",
              "3                      My girlfriend use quite often     0\n",
              "4                  Arrived as described. Very happy.     0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "TcOT8t5ZLblb",
        "outputId": "8da7c699-d4cf-4142-a167-d1f508bc6171"
      },
      "source": [
        "data.dropna(how='all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Crocheting for Dummies by Karen Manthey &amp; Susa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very helpful...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EASY TO UNDERSTAND AND A PROMPT SERVICE TOO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My girlfriend use quite often</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Arrived as described. Very happy.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>use it</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>I saw people learning the uke instantly at my ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>I didn't think anyone could screw up a ukulele...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>I am really happy with this started pack. It h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>My hubby and I decided we wanted to learn to p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            reviewText label\n",
              "0    Crocheting for Dummies by Karen Manthey & Susa...     0\n",
              "1                                      Very helpful...     0\n",
              "2          EASY TO UNDERSTAND AND A PROMPT SERVICE TOO     0\n",
              "3                        My girlfriend use quite often     0\n",
              "4                    Arrived as described. Very happy.     0\n",
              "..                                                 ...   ...\n",
              "795                                             use it     0\n",
              "796  I saw people learning the uke instantly at my ...     0\n",
              "797  I didn't think anyone could screw up a ukulele...     0\n",
              "798  I am really happy with this started pack. It h...     0\n",
              "799  My hubby and I decided we wanted to learn to p...     0\n",
              "\n",
              "[800 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hKayp-YskbT8",
        "outputId": "d4ed328d-ca38-4f4b-d144-e3c652e50227"
      },
      "source": [
        "data = data.append(data2, ignore_index= True )\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Crocheting for Dummies by Karen Manthey &amp; Susa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very helpful...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EASY TO UNDERSTAND AND A PROMPT SERVICE TOO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My girlfriend use quite often</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Arrived as described. Very happy.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>This is a very good quality product. The soun...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>This case is well made and it has a price to it.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>This is a great value, and it was easy to fin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>I am using these to set up. They work well, b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>I got this for my 2 year old niece and she lo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             reviewText label\n",
              "0     Crocheting for Dummies by Karen Manthey & Susa...     0\n",
              "1                                       Very helpful...     0\n",
              "2           EASY TO UNDERSTAND AND A PROMPT SERVICE TOO     0\n",
              "3                         My girlfriend use quite often     0\n",
              "4                     Arrived as described. Very happy.     0\n",
              "...                                                 ...   ...\n",
              "1595   This is a very good quality product. The soun...     1\n",
              "1596   This case is well made and it has a price to it.     1\n",
              "1597   This is a great value, and it was easy to fin...     1\n",
              "1598   I am using these to set up. They work well, b...     1\n",
              "1599   I got this for my 2 year old niece and she lo...     1\n",
              "\n",
              "[1600 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPpiCgUelN7O",
        "outputId": "d870d688-0a02-47a0-f7ad-9106e7b7b593"
      },
      "source": [
        "sentences=data.reviewText.values\n",
        "labels = data.label.values\n",
        "print(labels.shape)\n",
        "print(sentences.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1600,)\n",
            "(1600,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwtuevCsEL-b",
        "outputId": "5daded55-db53-4143-b684-405737c9044c"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# using the low level BERT for our task.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Printing the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Printing the tokenized sentence in form of list.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Crocheting for Dummies by Karen Manthey & Susan Brittain is a wonderfully thorough and very informative book for anyone wanting to learn to crochet and or wanting to freshen up their skills.\n",
            "\n",
            "The book reads like a storybook in paragraph form.  Everything is explained in great detail from choosing yarns and hooks, to how to work a large array of crochet stitches, to how to read a pattern, right down to how to care for ones crocheted items.\n",
            "\n",
            "The stitch drawings are clear and expertly done making learning new stitches so much easier.\n",
            "\n",
            "The book has both a contents page and an index for easy referral.  I especially liked the fact that an index was included.  So many crochet books do not include this.  The index makes it very easy to find information on a particular topic quickly.\n",
            "\n",
            "The recommendations for people just learning to crochet are fantastic.  This book wasn't out when I learned to crochet and I learned the hard way about many of the pit falls this book helps one to avoid.  For instance they recommend one start out with a size H-8 crochet hook and a light colored worsted weight yarn.  I learned with a B-1 hook and a fingering weight yarn.  After 2 whole days of crocheting it was 36\" long and 1.5\" tall.  I was trying to make a baby blanket for my doll (which never got made).\n",
            "\n",
            "The book contains humor, not just in the cartoons but in the instructions as well which makes for very entertaining reading while one learns a new craft.  I always appreciate having a teacher with a sense of humor!\n",
            "\n",
            "A good sampling of designs is included so that one can try out their skills.  These include sweaters, an afghan, doilies, hot pads, pillow, scarves, floral motifs, and bandanas.\n",
            "\n",
            "I am a crochet designer and I read the book cover to cover like a storybook while on vacation this past week.  I thoroughly enjoyed it and learned a few things as well.  I would highly recommend this book to anyone interested in the art of crochet.\n",
            "Tokenized:  ['cr', '##oche', '##ting', 'for', 'du', '##mm', '##ies', 'by', 'karen', 'man', '##the', '##y', '&', 'susan', 'brit', '##tain', 'is', 'a', 'wonderful', '##ly', 'thorough', 'and', 'very', 'inform', '##ative', 'book', 'for', 'anyone', 'wanting', 'to', 'learn', 'to', 'cr', '##oche', '##t', 'and', 'or', 'wanting', 'to', 'fresh', '##en', 'up', 'their', 'skills', '.', 'the', 'book', 'reads', 'like', 'a', 'story', '##book', 'in', 'paragraph', 'form', '.', 'everything', 'is', 'explained', 'in', 'great', 'detail', 'from', 'choosing', 'yarn', '##s', 'and', 'hooks', ',', 'to', 'how', 'to', 'work', 'a', 'large', 'array', 'of', 'cr', '##oche', '##t', 'stitches', ',', 'to', 'how', 'to', 'read', 'a', 'pattern', ',', 'right', 'down', 'to', 'how', 'to', 'care', 'for', 'ones', 'cr', '##oche', '##ted', 'items', '.', 'the', 'stitch', 'drawings', 'are', 'clear', 'and', 'expert', '##ly', 'done', 'making', 'learning', 'new', 'stitches', 'so', 'much', 'easier', '.', 'the', 'book', 'has', 'both', 'a', 'contents', 'page', 'and', 'an', 'index', 'for', 'easy', 'refer', '##ral', '.', 'i', 'especially', 'liked', 'the', 'fact', 'that', 'an', 'index', 'was', 'included', '.', 'so', 'many', 'cr', '##oche', '##t', 'books', 'do', 'not', 'include', 'this', '.', 'the', 'index', 'makes', 'it', 'very', 'easy', 'to', 'find', 'information', 'on', 'a', 'particular', 'topic', 'quickly', '.', 'the', 'recommendations', 'for', 'people', 'just', 'learning', 'to', 'cr', '##oche', '##t', 'are', 'fantastic', '.', 'this', 'book', 'wasn', \"'\", 't', 'out', 'when', 'i', 'learned', 'to', 'cr', '##oche', '##t', 'and', 'i', 'learned', 'the', 'hard', 'way', 'about', 'many', 'of', 'the', 'pit', 'falls', 'this', 'book', 'helps', 'one', 'to', 'avoid', '.', 'for', 'instance', 'they', 'recommend', 'one', 'start', 'out', 'with', 'a', 'size', 'h', '-', '8', 'cr', '##oche', '##t', 'hook', 'and', 'a', 'light', 'colored', 'worst', '##ed', 'weight', 'yarn', '.', 'i', 'learned', 'with', 'a', 'b', '-', '1', 'hook', 'and', 'a', 'finger', '##ing', 'weight', 'yarn', '.', 'after', '2', 'whole', 'days', 'of', 'cr', '##oche', '##ting', 'it', 'was', '36', '\"', 'long', 'and', '1', '.', '5', '\"', 'tall', '.', 'i', 'was', 'trying', 'to', 'make', 'a', 'baby', 'blanket', 'for', 'my', 'doll', '(', 'which', 'never', 'got', 'made', ')', '.', 'the', 'book', 'contains', 'humor', ',', 'not', 'just', 'in', 'the', 'cartoons', 'but', 'in', 'the', 'instructions', 'as', 'well', 'which', 'makes', 'for', 'very', 'entertaining', 'reading', 'while', 'one', 'learns', 'a', 'new', 'craft', '.', 'i', 'always', 'appreciate', 'having', 'a', 'teacher', 'with', 'a', 'sense', 'of', 'humor', '!', 'a', 'good', 'sampling', 'of', 'designs', 'is', 'included', 'so', 'that', 'one', 'can', 'try', 'out', 'their', 'skills', '.', 'these', 'include', 'sweater', '##s', ',', 'an', 'afghan', ',', 'doi', '##lies', ',', 'hot', 'pads', ',', 'pillow', ',', 'scar', '##ves', ',', 'floral', 'motifs', ',', 'and', 'banda', '##nas', '.', 'i', 'am', 'a', 'cr', '##oche', '##t', 'designer', 'and', 'i', 'read', 'the', 'book', 'cover', 'to', 'cover', 'like', 'a', 'story', '##book', 'while', 'on', 'vacation', 'this', 'past', 'week', '.', 'i', 'thoroughly', 'enjoyed', 'it', 'and', 'learned', 'a', 'few', 'things', 'as', 'well', '.', 'i', 'would', 'highly', 'recommend', 'this', 'book', 'to', 'anyone', 'interested', 'in', 'the', 'art', 'of', 'cr', '##oche', '##t', '.']\n",
            "Token IDs:  [13675, 23555, 3436, 2005, 4241, 7382, 3111, 2011, 8129, 2158, 10760, 2100, 1004, 6294, 28101, 18249, 2003, 1037, 6919, 2135, 16030, 1998, 2200, 12367, 8082, 2338, 2005, 3087, 5782, 2000, 4553, 2000, 13675, 23555, 2102, 1998, 2030, 5782, 2000, 4840, 2368, 2039, 2037, 4813, 1012, 1996, 2338, 9631, 2066, 1037, 2466, 8654, 1999, 20423, 2433, 1012, 2673, 2003, 4541, 1999, 2307, 6987, 2013, 10549, 27158, 2015, 1998, 18008, 1010, 2000, 2129, 2000, 2147, 1037, 2312, 9140, 1997, 13675, 23555, 2102, 25343, 1010, 2000, 2129, 2000, 3191, 1037, 5418, 1010, 2157, 2091, 2000, 2129, 2000, 2729, 2005, 3924, 13675, 23555, 3064, 5167, 1012, 1996, 26035, 9254, 2024, 3154, 1998, 6739, 2135, 2589, 2437, 4083, 2047, 25343, 2061, 2172, 6082, 1012, 1996, 2338, 2038, 2119, 1037, 8417, 3931, 1998, 2019, 5950, 2005, 3733, 6523, 7941, 1012, 1045, 2926, 4669, 1996, 2755, 2008, 2019, 5950, 2001, 2443, 1012, 2061, 2116, 13675, 23555, 2102, 2808, 2079, 2025, 2421, 2023, 1012, 1996, 5950, 3084, 2009, 2200, 3733, 2000, 2424, 2592, 2006, 1037, 3327, 8476, 2855, 1012, 1996, 11433, 2005, 2111, 2074, 4083, 2000, 13675, 23555, 2102, 2024, 10392, 1012, 2023, 2338, 2347, 1005, 1056, 2041, 2043, 1045, 4342, 2000, 13675, 23555, 2102, 1998, 1045, 4342, 1996, 2524, 2126, 2055, 2116, 1997, 1996, 6770, 4212, 2023, 2338, 7126, 2028, 2000, 4468, 1012, 2005, 6013, 2027, 16755, 2028, 2707, 2041, 2007, 1037, 2946, 1044, 1011, 1022, 13675, 23555, 2102, 8103, 1998, 1037, 2422, 6910, 5409, 2098, 3635, 27158, 1012, 1045, 4342, 2007, 1037, 1038, 1011, 1015, 8103, 1998, 1037, 4344, 2075, 3635, 27158, 1012, 2044, 1016, 2878, 2420, 1997, 13675, 23555, 3436, 2009, 2001, 4029, 1000, 2146, 1998, 1015, 1012, 1019, 1000, 4206, 1012, 1045, 2001, 2667, 2000, 2191, 1037, 3336, 8768, 2005, 2026, 10658, 1006, 2029, 2196, 2288, 2081, 1007, 1012, 1996, 2338, 3397, 8562, 1010, 2025, 2074, 1999, 1996, 13941, 2021, 1999, 1996, 8128, 2004, 2092, 2029, 3084, 2005, 2200, 14036, 3752, 2096, 2028, 10229, 1037, 2047, 7477, 1012, 1045, 2467, 9120, 2383, 1037, 3836, 2007, 1037, 3168, 1997, 8562, 999, 1037, 2204, 16227, 1997, 5617, 2003, 2443, 2061, 2008, 2028, 2064, 3046, 2041, 2037, 4813, 1012, 2122, 2421, 14329, 2015, 1010, 2019, 12632, 1010, 9193, 11983, 1010, 2980, 19586, 1010, 10005, 1010, 11228, 6961, 1010, 18686, 17366, 1010, 1998, 24112, 11649, 1012, 1045, 2572, 1037, 13675, 23555, 2102, 5859, 1998, 1045, 3191, 1996, 2338, 3104, 2000, 3104, 2066, 1037, 2466, 8654, 2096, 2006, 10885, 2023, 2627, 2733, 1012, 1045, 12246, 5632, 2009, 1998, 4342, 1037, 2261, 2477, 2004, 2092, 1012, 1045, 2052, 3811, 16755, 2023, 2338, 2000, 3087, 4699, 1999, 1996, 2396, 1997, 13675, 23555, 2102, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyoajnGWENQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e512ff1-9855-4734-b09e-61dd3948bd9a"
      },
      "source": [
        "input_ids = []\n",
        "for sent in sentences:\n",
        "    # so basically encode tokenizing , mapping sentences to thier token ids after adding special tokens.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        str(sent),                  # Sentence which are encoding.\n",
        "                        add_special_tokens = True,\n",
        "                        truncation = True ,\n",
        "                        max_length = 608 # Adding special tokens '[CLS]' and '[SEP]'\n",
        "\n",
        "                         )\n",
        "    \n",
        " \n",
        "    input_ids.append(encoded_sent)\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Crocheting for Dummies by Karen Manthey & Susan Brittain is a wonderfully thorough and very informative book for anyone wanting to learn to crochet and or wanting to freshen up their skills.\n",
            "\n",
            "The book reads like a storybook in paragraph form.  Everything is explained in great detail from choosing yarns and hooks, to how to work a large array of crochet stitches, to how to read a pattern, right down to how to care for ones crocheted items.\n",
            "\n",
            "The stitch drawings are clear and expertly done making learning new stitches so much easier.\n",
            "\n",
            "The book has both a contents page and an index for easy referral.  I especially liked the fact that an index was included.  So many crochet books do not include this.  The index makes it very easy to find information on a particular topic quickly.\n",
            "\n",
            "The recommendations for people just learning to crochet are fantastic.  This book wasn't out when I learned to crochet and I learned the hard way about many of the pit falls this book helps one to avoid.  For instance they recommend one start out with a size H-8 crochet hook and a light colored worsted weight yarn.  I learned with a B-1 hook and a fingering weight yarn.  After 2 whole days of crocheting it was 36\" long and 1.5\" tall.  I was trying to make a baby blanket for my doll (which never got made).\n",
            "\n",
            "The book contains humor, not just in the cartoons but in the instructions as well which makes for very entertaining reading while one learns a new craft.  I always appreciate having a teacher with a sense of humor!\n",
            "\n",
            "A good sampling of designs is included so that one can try out their skills.  These include sweaters, an afghan, doilies, hot pads, pillow, scarves, floral motifs, and bandanas.\n",
            "\n",
            "I am a crochet designer and I read the book cover to cover like a storybook while on vacation this past week.  I thoroughly enjoyed it and learned a few things as well.  I would highly recommend this book to anyone interested in the art of crochet.\n",
            "Token IDs: [101, 13675, 23555, 3436, 2005, 4241, 7382, 3111, 2011, 8129, 2158, 10760, 2100, 1004, 6294, 28101, 18249, 2003, 1037, 6919, 2135, 16030, 1998, 2200, 12367, 8082, 2338, 2005, 3087, 5782, 2000, 4553, 2000, 13675, 23555, 2102, 1998, 2030, 5782, 2000, 4840, 2368, 2039, 2037, 4813, 1012, 1996, 2338, 9631, 2066, 1037, 2466, 8654, 1999, 20423, 2433, 1012, 2673, 2003, 4541, 1999, 2307, 6987, 2013, 10549, 27158, 2015, 1998, 18008, 1010, 2000, 2129, 2000, 2147, 1037, 2312, 9140, 1997, 13675, 23555, 2102, 25343, 1010, 2000, 2129, 2000, 3191, 1037, 5418, 1010, 2157, 2091, 2000, 2129, 2000, 2729, 2005, 3924, 13675, 23555, 3064, 5167, 1012, 1996, 26035, 9254, 2024, 3154, 1998, 6739, 2135, 2589, 2437, 4083, 2047, 25343, 2061, 2172, 6082, 1012, 1996, 2338, 2038, 2119, 1037, 8417, 3931, 1998, 2019, 5950, 2005, 3733, 6523, 7941, 1012, 1045, 2926, 4669, 1996, 2755, 2008, 2019, 5950, 2001, 2443, 1012, 2061, 2116, 13675, 23555, 2102, 2808, 2079, 2025, 2421, 2023, 1012, 1996, 5950, 3084, 2009, 2200, 3733, 2000, 2424, 2592, 2006, 1037, 3327, 8476, 2855, 1012, 1996, 11433, 2005, 2111, 2074, 4083, 2000, 13675, 23555, 2102, 2024, 10392, 1012, 2023, 2338, 2347, 1005, 1056, 2041, 2043, 1045, 4342, 2000, 13675, 23555, 2102, 1998, 1045, 4342, 1996, 2524, 2126, 2055, 2116, 1997, 1996, 6770, 4212, 2023, 2338, 7126, 2028, 2000, 4468, 1012, 2005, 6013, 2027, 16755, 2028, 2707, 2041, 2007, 1037, 2946, 1044, 1011, 1022, 13675, 23555, 2102, 8103, 1998, 1037, 2422, 6910, 5409, 2098, 3635, 27158, 1012, 1045, 4342, 2007, 1037, 1038, 1011, 1015, 8103, 1998, 1037, 4344, 2075, 3635, 27158, 1012, 2044, 1016, 2878, 2420, 1997, 13675, 23555, 3436, 2009, 2001, 4029, 1000, 2146, 1998, 1015, 1012, 1019, 1000, 4206, 1012, 1045, 2001, 2667, 2000, 2191, 1037, 3336, 8768, 2005, 2026, 10658, 1006, 2029, 2196, 2288, 2081, 1007, 1012, 1996, 2338, 3397, 8562, 1010, 2025, 2074, 1999, 1996, 13941, 2021, 1999, 1996, 8128, 2004, 2092, 2029, 3084, 2005, 2200, 14036, 3752, 2096, 2028, 10229, 1037, 2047, 7477, 1012, 1045, 2467, 9120, 2383, 1037, 3836, 2007, 1037, 3168, 1997, 8562, 999, 1037, 2204, 16227, 1997, 5617, 2003, 2443, 2061, 2008, 2028, 2064, 3046, 2041, 2037, 4813, 1012, 2122, 2421, 14329, 2015, 1010, 2019, 12632, 1010, 9193, 11983, 1010, 2980, 19586, 1010, 10005, 1010, 11228, 6961, 1010, 18686, 17366, 1010, 1998, 24112, 11649, 1012, 1045, 2572, 1037, 13675, 23555, 2102, 5859, 1998, 1045, 3191, 1996, 2338, 3104, 2000, 3104, 2066, 1037, 2466, 8654, 2096, 2006, 10885, 2023, 2627, 2733, 1012, 1045, 12246, 5632, 2009, 1998, 4342, 1037, 2261, 2477, 2004, 2092, 1012, 1045, 2052, 3811, 16755, 2023, 2338, 2000, 3087, 4699, 1999, 1996, 2396, 1997, 13675, 23555, 2102, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt16hndyEOzy"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 64\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN , truncating=\"post\", padding=\"post\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28GiLtVMEP6Q"
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Generating attention mask for sentences.\n",
        "    #   - when there is 0 present as token id we are going to set mask as 0.\n",
        "    #   - we are going to set mask 1 for all non-zero positive input id.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "   \n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywqAi6LvESp3",
        "outputId": "6977f556-96ea-4646-d9ae-8d5f78bc300f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np \n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "print(train_labels.dtype)\n",
        "train_labels = train_labels.astype(np.float32)\n",
        "validation_labels = validation_labels.astype(np.float32)\n",
        "#changing the numpy arrays into tensors for working on GPU. \n",
        "\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels).type(torch.long)\n",
        "validation_labels = torch.tensor(validation_labels).type(torch.long)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Deciding the batch size for training.\n",
        "\n",
        "batch_size = 1\n",
        "print(train_inputs.size())\n",
        "print(train_masks.size())\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object\n",
            "torch.Size([1440, 64])\n",
            "torch.Size([1440, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lueyO5msEUaU",
        "outputId": "5d14c437-5b10-452e-e7d7-b5937e6c7f57"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    return_dict=True\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGYIA78htHas",
        "outputId": "cf51990b-ba32-4a67-ff4e-86f401a689e9"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9REkOuRxEVzD"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-__LSC0OtO01"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvSkGMMptRCu"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZCskMcTtRIm"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAKmKVzOEXVe",
        "outputId": "a71c5e29-375b-49a0-c977-f84a04a78777"
      },
      "source": [
        "import random\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.   Loss: {:}'.format(step, len(train_dataloader), elapsed, total_loss))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(\n",
        "            b_input_ids, \n",
        "            token_type_ids=None, \n",
        "            attention_mask=b_input_mask,\n",
        "            labels = b_labels\n",
        "        )\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,440.    Elapsed: 0:00:05.   Loss: 35.259660854935646\n",
            "  Batch    80  of  1,440.    Elapsed: 0:00:10.   Loss: 62.698383167386055\n",
            "  Batch   120  of  1,440.    Elapsed: 0:00:15.   Loss: 90.40916799753904\n",
            "  Batch   160  of  1,440.    Elapsed: 0:00:20.   Loss: 108.57296766154468\n",
            "  Batch   200  of  1,440.    Elapsed: 0:00:25.   Loss: 134.48613845417276\n",
            "  Batch   240  of  1,440.    Elapsed: 0:00:31.   Loss: 150.7231056872988\n",
            "  Batch   280  of  1,440.    Elapsed: 0:00:36.   Loss: 170.4160819313256\n",
            "  Batch   320  of  1,440.    Elapsed: 0:00:41.   Loss: 195.51072293822654\n",
            "  Batch   360  of  1,440.    Elapsed: 0:00:46.   Loss: 206.36657705437392\n",
            "  Batch   400  of  1,440.    Elapsed: 0:00:51.   Loss: 237.4117173505947\n",
            "  Batch   440  of  1,440.    Elapsed: 0:00:56.   Loss: 262.0179202008876\n",
            "  Batch   480  of  1,440.    Elapsed: 0:01:01.   Loss: 276.382289016532\n",
            "  Batch   520  of  1,440.    Elapsed: 0:01:06.   Loss: 286.13375305209775\n",
            "  Batch   560  of  1,440.    Elapsed: 0:01:11.   Loss: 301.8490179644141\n",
            "  Batch   600  of  1,440.    Elapsed: 0:01:16.   Loss: 326.8516711818229\n",
            "  Batch   640  of  1,440.    Elapsed: 0:01:21.   Loss: 348.51926463798736\n",
            "  Batch   680  of  1,440.    Elapsed: 0:01:26.   Loss: 381.29666629523854\n",
            "  Batch   720  of  1,440.    Elapsed: 0:01:31.   Loss: 406.59736880558194\n",
            "  Batch   760  of  1,440.    Elapsed: 0:01:36.   Loss: 429.96063330658944\n",
            "  Batch   800  of  1,440.    Elapsed: 0:01:41.   Loss: 447.6381786119018\n",
            "  Batch   840  of  1,440.    Elapsed: 0:01:47.   Loss: 475.614615303406\n",
            "  Batch   880  of  1,440.    Elapsed: 0:01:52.   Loss: 508.68987599009415\n",
            "  Batch   920  of  1,440.    Elapsed: 0:01:57.   Loss: 518.2499550177599\n",
            "  Batch   960  of  1,440.    Elapsed: 0:02:02.   Loss: 532.8774582090264\n",
            "  Batch 1,000  of  1,440.    Elapsed: 0:02:07.   Loss: 561.353716964979\n",
            "  Batch 1,040  of  1,440.    Elapsed: 0:02:12.   Loss: 582.2493696337333\n",
            "  Batch 1,080  of  1,440.    Elapsed: 0:02:17.   Loss: 597.088281834469\n",
            "  Batch 1,120  of  1,440.    Elapsed: 0:02:22.   Loss: 604.5530524131609\n",
            "  Batch 1,160  of  1,440.    Elapsed: 0:02:27.   Loss: 618.6402563121228\n",
            "  Batch 1,200  of  1,440.    Elapsed: 0:02:32.   Loss: 634.4790925014531\n",
            "  Batch 1,240  of  1,440.    Elapsed: 0:02:37.   Loss: 656.7509222895897\n",
            "  Batch 1,280  of  1,440.    Elapsed: 0:02:42.   Loss: 679.5216499617964\n",
            "  Batch 1,320  of  1,440.    Elapsed: 0:02:47.   Loss: 694.4355056783243\n",
            "  Batch 1,360  of  1,440.    Elapsed: 0:02:52.   Loss: 708.2942953618476\n",
            "  Batch 1,400  of  1,440.    Elapsed: 0:02:58.   Loss: 726.9372681294335\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:03:03\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,440.    Elapsed: 0:00:05.   Loss: 0.18927191573311575\n",
            "  Batch    80  of  1,440.    Elapsed: 0:00:10.   Loss: 10.542369861068437\n",
            "  Batch   120  of  1,440.    Elapsed: 0:00:15.   Loss: 19.32363988040015\n",
            "  Batch   160  of  1,440.    Elapsed: 0:00:20.   Loss: 19.34180035223835\n",
            "  Batch   200  of  1,440.    Elapsed: 0:00:25.   Loss: 33.58024682964606\n",
            "  Batch   240  of  1,440.    Elapsed: 0:00:30.   Loss: 33.60530825330352\n",
            "  Batch   280  of  1,440.    Elapsed: 0:00:35.   Loss: 49.29722627087904\n",
            "  Batch   320  of  1,440.    Elapsed: 0:00:40.   Loss: 49.352790036195074\n",
            "  Batch   360  of  1,440.    Elapsed: 0:00:46.   Loss: 49.40117140187067\n",
            "  Batch   400  of  1,440.    Elapsed: 0:00:51.   Loss: 71.9476170859707\n",
            "  Batch   440  of  1,440.    Elapsed: 0:00:56.   Loss: 88.1918492315599\n",
            "  Batch   480  of  1,440.    Elapsed: 0:01:01.   Loss: 97.9868100737367\n",
            "  Batch   520  of  1,440.    Elapsed: 0:01:06.   Loss: 98.0242689348961\n",
            "  Batch   560  of  1,440.    Elapsed: 0:01:11.   Loss: 124.61067537374038\n",
            "  Batch   600  of  1,440.    Elapsed: 0:01:16.   Loss: 134.61567483689578\n",
            "  Batch   640  of  1,440.    Elapsed: 0:01:21.   Loss: 153.52629164456448\n",
            "  Batch   680  of  1,440.    Elapsed: 0:01:26.   Loss: 161.0009639501659\n",
            "  Batch   720  of  1,440.    Elapsed: 0:01:31.   Loss: 161.15097769827116\n",
            "  Batch   760  of  1,440.    Elapsed: 0:01:36.   Loss: 179.60170797928004\n",
            "  Batch   800  of  1,440.    Elapsed: 0:01:41.   Loss: 194.2321418099309\n",
            "  Batch   840  of  1,440.    Elapsed: 0:01:46.   Loss: 209.01235550707497\n",
            "  Batch   880  of  1,440.    Elapsed: 0:01:52.   Loss: 209.05142612929922\n",
            "  Batch   920  of  1,440.    Elapsed: 0:01:57.   Loss: 218.71211047108227\n",
            "  Batch   960  of  1,440.    Elapsed: 0:02:02.   Loss: 233.35270893132838\n",
            "  Batch 1,000  of  1,440.    Elapsed: 0:02:07.   Loss: 251.66352134347835\n",
            "  Batch 1,040  of  1,440.    Elapsed: 0:02:12.   Loss: 269.3879830810765\n",
            "  Batch 1,080  of  1,440.    Elapsed: 0:02:17.   Loss: 277.1311721721577\n",
            "  Batch 1,120  of  1,440.    Elapsed: 0:02:22.   Loss: 293.44937157735694\n",
            "  Batch 1,160  of  1,440.    Elapsed: 0:02:27.   Loss: 297.348322738515\n",
            "  Batch 1,200  of  1,440.    Elapsed: 0:02:32.   Loss: 315.9524882775295\n",
            "  Batch 1,240  of  1,440.    Elapsed: 0:02:37.   Loss: 332.4777475139563\n",
            "  Batch 1,280  of  1,440.    Elapsed: 0:02:42.   Loss: 332.5335040118807\n",
            "  Batch 1,320  of  1,440.    Elapsed: 0:02:47.   Loss: 348.67007461506\n",
            "  Batch 1,360  of  1,440.    Elapsed: 0:02:52.   Loss: 355.88644038769417\n",
            "  Batch 1,400  of  1,440.    Elapsed: 0:02:57.   Loss: 384.16824991547037\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:03:03\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,440.    Elapsed: 0:00:05.   Loss: 2.152732343383832\n",
            "  Batch    80  of  1,440.    Elapsed: 0:00:10.   Loss: 8.388900745063438\n",
            "  Batch   120  of  1,440.    Elapsed: 0:00:15.   Loss: 8.404942059947643\n",
            "  Batch   160  of  1,440.    Elapsed: 0:00:20.   Loss: 8.452581694160472\n",
            "  Batch   200  of  1,440.    Elapsed: 0:00:25.   Loss: 15.455249725768226\n",
            "  Batch   240  of  1,440.    Elapsed: 0:00:30.   Loss: 27.845527808698534\n",
            "  Batch   280  of  1,440.    Elapsed: 0:00:35.   Loss: 27.860777305388183\n",
            "  Batch   320  of  1,440.    Elapsed: 0:00:41.   Loss: 29.95950157660991\n",
            "  Batch   360  of  1,440.    Elapsed: 0:00:46.   Loss: 29.972433725866722\n",
            "  Batch   400  of  1,440.    Elapsed: 0:00:51.   Loss: 43.553130449821765\n",
            "  Batch   440  of  1,440.    Elapsed: 0:00:56.   Loss: 43.59371297950565\n",
            "  Batch   480  of  1,440.    Elapsed: 0:01:01.   Loss: 50.34318464390526\n",
            "  Batch   520  of  1,440.    Elapsed: 0:01:06.   Loss: 63.72492247448099\n",
            "  Batch   560  of  1,440.    Elapsed: 0:01:11.   Loss: 80.7428485265118\n",
            "  Batch   600  of  1,440.    Elapsed: 0:01:16.   Loss: 87.28222253198328\n",
            "  Batch   640  of  1,440.    Elapsed: 0:01:21.   Loss: 87.3015512988859\n",
            "  Batch   680  of  1,440.    Elapsed: 0:01:26.   Loss: 94.14023697050288\n",
            "  Batch   720  of  1,440.    Elapsed: 0:01:31.   Loss: 110.36710257632512\n",
            "  Batch   760  of  1,440.    Elapsed: 0:01:36.   Loss: 110.3877425318642\n",
            "  Batch   800  of  1,440.    Elapsed: 0:01:41.   Loss: 117.17033008587896\n",
            "  Batch   840  of  1,440.    Elapsed: 0:01:47.   Loss: 117.18206356486189\n",
            "  Batch   880  of  1,440.    Elapsed: 0:01:52.   Loss: 125.44380559377896\n",
            "  Batch   920  of  1,440.    Elapsed: 0:01:57.   Loss: 125.46077785875968\n",
            "  Batch   960  of  1,440.    Elapsed: 0:02:02.   Loss: 137.10763998113543\n",
            "  Batch 1,000  of  1,440.    Elapsed: 0:02:07.   Loss: 142.03798781722435\n",
            "  Batch 1,040  of  1,440.    Elapsed: 0:02:12.   Loss: 142.05168045225582\n",
            "  Batch 1,080  of  1,440.    Elapsed: 0:02:17.   Loss: 142.06806234792748\n",
            "  Batch 1,120  of  1,440.    Elapsed: 0:02:22.   Loss: 166.92860099290556\n",
            "  Batch 1,160  of  1,440.    Elapsed: 0:02:27.   Loss: 166.94359870002518\n",
            "  Batch 1,200  of  1,440.    Elapsed: 0:02:32.   Loss: 176.09602456993161\n",
            "  Batch 1,240  of  1,440.    Elapsed: 0:02:37.   Loss: 181.1978177017736\n",
            "  Batch 1,280  of  1,440.    Elapsed: 0:02:42.   Loss: 189.15855062294577\n",
            "  Batch 1,320  of  1,440.    Elapsed: 0:02:47.   Loss: 193.273339768828\n",
            "  Batch 1,360  of  1,440.    Elapsed: 0:02:52.   Loss: 209.63744435256376\n",
            "  Batch 1,400  of  1,440.    Elapsed: 0:02:58.   Loss: 209.64761843631277\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:03:03\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,440.    Elapsed: 0:00:05.   Loss: 4.502502115145035\n",
            "  Batch    80  of  1,440.    Elapsed: 0:00:10.   Loss: 10.643071322156175\n",
            "  Batch   120  of  1,440.    Elapsed: 0:00:15.   Loss: 15.640950831097143\n",
            "  Batch   160  of  1,440.    Elapsed: 0:00:20.   Loss: 15.647994389000814\n",
            "  Batch   200  of  1,440.    Elapsed: 0:00:25.   Loss: 15.657338062814233\n",
            "  Batch   240  of  1,440.    Elapsed: 0:00:30.   Loss: 15.662466611258424\n",
            "  Batch   280  of  1,440.    Elapsed: 0:00:35.   Loss: 15.672055572693353\n",
            "  Batch   320  of  1,440.    Elapsed: 0:00:41.   Loss: 21.27101325149124\n",
            "  Batch   360  of  1,440.    Elapsed: 0:00:46.   Loss: 26.64206333654147\n",
            "  Batch   400  of  1,440.    Elapsed: 0:00:51.   Loss: 26.646975077870593\n",
            "  Batch   440  of  1,440.    Elapsed: 0:00:56.   Loss: 30.26480767992325\n",
            "  Batch   480  of  1,440.    Elapsed: 0:01:01.   Loss: 30.278073115718144\n",
            "  Batch   520  of  1,440.    Elapsed: 0:01:06.   Loss: 35.39232056761466\n",
            "  Batch   560  of  1,440.    Elapsed: 0:01:11.   Loss: 43.488364255612396\n",
            "  Batch   600  of  1,440.    Elapsed: 0:01:16.   Loss: 51.29765936813783\n",
            "  Batch   640  of  1,440.    Elapsed: 0:01:21.   Loss: 51.30725119640556\n",
            "  Batch   680  of  1,440.    Elapsed: 0:01:26.   Loss: 57.16649906943712\n",
            "  Batch   720  of  1,440.    Elapsed: 0:01:31.   Loss: 58.778186195209855\n",
            "  Batch   760  of  1,440.    Elapsed: 0:01:36.   Loss: 58.7929216630946\n",
            "  Batch   800  of  1,440.    Elapsed: 0:01:41.   Loss: 58.79947768627244\n",
            "  Batch   840  of  1,440.    Elapsed: 0:01:46.   Loss: 61.617004902709596\n",
            "  Batch   880  of  1,440.    Elapsed: 0:01:52.   Loss: 73.48271570020734\n",
            "  Batch   920  of  1,440.    Elapsed: 0:01:57.   Loss: 73.48591047280206\n",
            "  Batch   960  of  1,440.    Elapsed: 0:02:02.   Loss: 73.49185591265632\n",
            "  Batch 1,000  of  1,440.    Elapsed: 0:02:07.   Loss: 73.49618355957136\n",
            "  Batch 1,040  of  1,440.    Elapsed: 0:02:12.   Loss: 73.50112740766053\n",
            "  Batch 1,080  of  1,440.    Elapsed: 0:02:17.   Loss: 73.50563898969995\n",
            "  Batch 1,120  of  1,440.    Elapsed: 0:02:22.   Loss: 88.86488298281256\n",
            "  Batch 1,160  of  1,440.    Elapsed: 0:02:27.   Loss: 88.87266667233416\n",
            "  Batch 1,200  of  1,440.    Elapsed: 0:02:32.   Loss: 88.95548372294797\n",
            "  Batch 1,240  of  1,440.    Elapsed: 0:02:37.   Loss: 88.9759020063284\n",
            "  Batch 1,280  of  1,440.    Elapsed: 0:02:42.   Loss: 91.97355043620337\n",
            "  Batch 1,320  of  1,440.    Elapsed: 0:02:47.   Loss: 91.97945934778545\n",
            "  Batch 1,360  of  1,440.    Elapsed: 0:02:52.   Loss: 97.07682222764197\n",
            "  Batch 1,400  of  1,440.    Elapsed: 0:02:57.   Loss: 97.08182826535631\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:03:03\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "QemYnW2jDx4E",
        "outputId": "dc2f3e9b-7a7d-4fa6-f451-1ed577f354be"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xHDACojDzuy",
        "outputId": "d246376c-8ea9-4f92-c873-1cc3680e5502"
      },
      "source": [
        "import pandas as pd\n",
        "testData=test.copy()\n",
        "testData2=test2.copy()\n",
        "testData2[\"label\"] = '1'\n",
        "testData.drop(['overall',\t'vote',\t'verified',\t'reviewTime',\t'reviewerID',\t'asin',\t'style',\t'reviewerName',\t'summary'\t,'unixReviewTime' ,\t'image'],axis=1,inplace=True)\n",
        "testData[\"label\"] = '0'\n",
        "testData.dropna(how='all')\n",
        "# Load the dataset into a pandas dataframe.\n",
        "testData[\"label\"] = pd.to_numeric(testData[\"label\"], downcast=\"float\")\n",
        "testData2[\"label\"] = pd.to_numeric(testData2[\"label\"], downcast=\"float\")\n",
        "df = testData.append(testData2, ignore_index= True )\n",
        "\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.reviewText.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "labels = labels.astype(np.float32)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1261 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twTzerhcFCpj",
        "outputId": "48e12cd1-c556-4b78-8017-ec01aac29d7f"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 400 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ILbV0BYFGqt",
        "outputId": "1d9ec9bb-0522-43bf-cb6c-9ddef62fce89"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples: 200 of 400 (50.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-lozmtdFHvh",
        "outputId": "deeaedbc-f8f0-4f52-9175-dd7ed984afcd"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhJjXVKDFirO",
        "outputId": "01b8dad9-fa81-4a30-e49e-afc8d266eab3"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 0.966\n"
          ]
        }
      ]
    }
  ]
}